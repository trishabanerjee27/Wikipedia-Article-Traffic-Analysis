{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Dependencies\n",
    "\n",
    "In this section, we begin by importing the necessary Python libraries and modules required for our project. The following standard and external libraries will be used:\n",
    "\n",
    "### Standard Python Libraries:\n",
    "- **json**: To handle JSON data.\n",
    "- **time**: Provides various time-related functions.\n",
    "- **urllib.parse**: To handle and manipulate URLs.\n",
    "\n",
    "### External Libraries:\n",
    "- **requests**: For making HTTP requests to fetch data from web APIs (you may need to install this package using `pip install requests`).\n",
    "- **pandas**: A powerful data manipulation and analysis library (can be installed using `pip install pandas`).\n",
    "- **datetime**: To handle date and time data, particularly useful for working with timestamps.\n",
    "- **matplotlib.pyplot**: A popular data visualization library for creating plots and charts (can be installed using `pip install matplotlib`).\n",
    "\n",
    "Ensure that all external libraries are installed before running the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are standard python modules\n",
    "import json, time, urllib.parse\n",
    "\n",
    "\n",
    "# The following modules are not standard Python modules. You will need to install this with pip/pip3 if you do not already have it\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and API Setup\n",
    "\n",
    "This section defines constants and parameters for interacting with the Wikimedia Pageviews API.\n",
    "\n",
    "### API URL and Parameters:\n",
    "- **`API_REQUEST_PAGEVIEWS_ENDPOINT`**: Base URL for Wikimedia 'pageviews' requests.\n",
    "- **`API_REQUEST_PER_ARTICLE_PARAMS`**: Template string for 'per-article' API requests, allowing customization of project, access type, article, date range, etc.\n",
    "\n",
    "### Rate Limiting:\n",
    "- **`API_LATENCY_ASSUMED`** & **`API_THROTTLE_WAIT`**: Ensure compliance with Wikimedia's rate limit (100 requests/second) by introducing a small delay.\n",
    "\n",
    "### User-Agent:\n",
    "- **`REQUEST_HEADERS`**: Includes contact details to comply with API requirements.\n",
    "\n",
    "### Article List:\n",
    "- **`ARTICLE_TITLES`**: Example articles like 'Bison' and 'Chinook salmon' for which we will fetch pageviews.\n",
    "\n",
    "### API Template:\n",
    "- **`ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE`**: Template dictionary to structure API parameters, with a set date range and customizable article names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The REST API 'pageviews' URL - this is the common URL/endpoint for all 'pageviews' API requests\n",
    "API_REQUEST_PAGEVIEWS_ENDPOINT = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/'\n",
    "\n",
    "# This is a parameterized string that specifies what kind of pageviews request we are going to make\n",
    "# In this case it will be a 'per-article' based request. The string is a format string so that we can\n",
    "# replace each parameter with an appropriate value before making the request\n",
    "API_REQUEST_PER_ARTICLE_PARAMS = 'per-article/{project}/{access}/{agent}/{article}/{granularity}/{start}/{end}'\n",
    "\n",
    "# The Pageviews API asks that we not exceed 100 requests per second, we add a small delay to each request\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making a request to the Wikimedia API they ask that you include your email address which will allow them\n",
    "# to contact you if something happens - such as - your code exceeding rate limits - or some other error \n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<tbaner@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2024',\n",
    "}\n",
    "\n",
    "# This is just a list of English Wikipedia article titles that we can use for example requests\n",
    "ARTICLE_TITLES = [ 'Bison', 'Northern flicker', 'Red squirrel', 'Chinook salmon', 'Horseshoe bat' ]\n",
    "\n",
    "# This template is used to map parameter values into the API_REQUST_PER_ARTICLE_PARAMS portion of an API request. The dictionary has a\n",
    "# field/key for each of the required parameters. In the example, below, we only vary the article name, so the majority of the fields\n",
    "# can stay constant for each request. Of course, these values *could* be changed if necessary.\n",
    "ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      \"desktop\",      # this should be changed for the different access types\n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     \"\",             # this value will be set/changed before each request\n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       \"2015070100\",\n",
    "    \"end\":         \"2024093000\" \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedures and Functions\n",
    "\n",
    "This section outlines the function that retrieves pageview data for individual Wikipedia articles using the Wikimedia Pageviews API.\n",
    "\n",
    "### `request_pageviews_per_article()`\n",
    "\n",
    "This function requests the monthly pageviews for a specified Wikipedia article. The function accepts parameters like the article title, access type (e.g., desktop, mobile), and constructs the full API request URL using predefined constants. Below is a breakdown of its key components:\n",
    "\n",
    "- **Function Parameters**:\n",
    "  - `article_title`: The title of the Wikipedia article (default is `None`). The function raises an exception if no article is provided.\n",
    "  - `access_type`: Specifies how the article was accessed (e.g., desktop or mobile; default is \"desktop\").\n",
    "  - `endpoint_url`: Base URL of the Wikimedia Pageviews API.\n",
    "  - `endpoint_params`: Template for the specific API request parameters.\n",
    "  - `request_template`: A dictionary containing request parameters, which are customized for each article.\n",
    "  - `headers`: HTTP headers, including User-Agent information, to identify the requester.\n",
    "\n",
    "- **Article Title Handling**:\n",
    "  - The function checks if an article title is provided. If so, it updates the `article` key in the `request_template` with the provided title.\n",
    "  - The article title is URL-encoded to handle spaces and special characters before being formatted into the request URL.\n",
    "\n",
    "- **Access Type**:\n",
    "  - The `access_type` (e.g., desktop, mobile-web) is set in the request template. This helps filter the pageviews based on how the article was accessed.\n",
    "\n",
    "- **URL Construction**:\n",
    "  - The function dynamically formats the URL by inserting values from the `request_template` into the API request template string. This constructs the final URL for the API request.\n",
    "\n",
    "- **Throttling**:\n",
    "  - To comply with the Wikimedia API’s rate limits, the function includes a small delay (`API_THROTTLE_WAIT`) before making each request. This ensures we don’t exceed the maximum allowed requests per second.\n",
    "\n",
    "- **Making the API Request**:\n",
    "  - The function uses `requests.get()` to send the API request. It includes error handling to catch and report exceptions during the request process.\n",
    "\n",
    "- **Response Handling**:\n",
    "  - If the request is successful, the function returns the API’s JSON response, which contains the pageview data. If there’s an error or exception, it prints the error and returns `None`.\n",
    "\n",
    "The function can be used in a loop to request pageview data for multiple articles by passing different article titles into the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "# Function to request pageviews per article\n",
    "def request_pageviews_per_article(article_title=None, \n",
    "                                  access_type=\"desktop\",\n",
    "                                  endpoint_url=API_REQUEST_PAGEVIEWS_ENDPOINT, \n",
    "                                  endpoint_params=API_REQUEST_PER_ARTICLE_PARAMS, \n",
    "                                  request_template=ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE,\n",
    "                                  headers=REQUEST_HEADERS):\n",
    "\n",
    "    # Set the article title\n",
    "    if article_title:\n",
    "        request_template['article'] = article_title\n",
    "\n",
    "    if not request_template['article']:\n",
    "        raise Exception(\"Must supply an article title to make a pageviews request.\")\n",
    "\n",
    "    # Set the access type\n",
    "    request_template['access'] = access_type\n",
    "    \n",
    "    # Encode the article title for URL\n",
    "    article_title_encoded = urllib.parse.quote(request_template['article'].replace(' ','_'))\n",
    "    request_template['article'] = article_title_encoded\n",
    "    \n",
    "    # Create the request URL\n",
    "    request_url = endpoint_url + endpoint_params.format(**request_template)\n",
    "    \n",
    "    # Make the request and handle exceptions\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Article Titles and Data Initialization\n",
    "\n",
    "In this section, we load article titles from a CSV file, which contains a column labeled 'disease' representing Wikipedia page titles. These titles will be used to request pageview data from the Wikimedia API. Additionally, we initialize empty lists to store the pageview data for desktop, mobile, and cumulative views.\n",
    "\n",
    "### Steps:\n",
    "- **Loading the CSV File**:\n",
    "  - We use `pandas` to load the CSV file `rare-disease_cleaned.AUG.2024.csv` and extract the 'disease' column into a list of article titles.\n",
    "\n",
    "- **Data Initialization**:\n",
    "  - We initialize three empty lists: `desktop_data`, `mobile_data`, and `cumulative_data`. These lists will store the pageview data for each article, categorized by access type (desktop and mobile). Cumulative data will represent the total pageviews from both access types combined.\n",
    "\n",
    "This setup allows us to loop through each article title and request pageview data using the `request_pageviews_per_article()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the article titles from the provided CSV file (column: 'disease')\n",
    "df_pages = pd.read_csv(\"../data/rare-disease_cleaned.AUG.2024.csv\")\n",
    "pages = df_pages['disease'].tolist()  # Convert the 'disease' column to a list of article titles\n",
    "\n",
    "# Initialize empty lists to store the collected data for desktop, mobile, and cumulative views\n",
    "desktop_data = []\n",
    "mobile_data = []\n",
    "cumulative_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Pageview Data for Each Article\n",
    "\n",
    "This section iterates over each article title from the `pages` list to request pageview data from the Wikimedia API. Data is collected for three access types: desktop, mobile (web + app), and cumulative views. For each article, the function stores the results in corresponding lists.\n",
    "\n",
    "### Steps:\n",
    "1. **Desktop Views**:\n",
    "   - Calls the `request_pageviews_per_article()` function to retrieve pageviews for desktop access.\n",
    "   - If data is available, it loops through each monthly entry and appends the article title, timestamp, and views to the `desktop_data` list.\n",
    "   - If no data is found for desktop, it prints a message indicating no data for that article.\n",
    "\n",
    "2. **Mobile Views (mobile-web + mobile-app)**:\n",
    "   - Fetches pageviews separately for mobile-web and mobile-app access types.\n",
    "   - The data for both access types is summed, and the combined views are stored in the `mobile_data` list.\n",
    "   - If no data is found for either mobile-web or mobile-app, a message is printed.\n",
    "\n",
    "3. **Cumulative Views (All Access Types)**:\n",
    "   - Retrieves cumulative pageviews across all access types.\n",
    "   - Each entry is stored in the `cumulative_data` list, including the article title, timestamp, and views.\n",
    "   - If no cumulative data is found, a message is printed for that article.\n",
    "\n",
    "This loop ensures that all access types are covered and that each request handles missing data gracefully by printing appropriate error messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for Sulfadoxine/pyrimethamine (desktop)\n",
      "No data for Sulfadoxine/pyrimethamine (mobile-web or mobile-app)\n",
      "No data for Sulfadoxine/pyrimethamine (all-access)\n",
      "No data for Cystine/glutamate transporter (desktop)\n",
      "No data for Cystine/glutamate transporter (mobile-web or mobile-app)\n",
      "No data for Cystine/glutamate transporter (all-access)\n",
      "No data for Trimethoprim/sulfamethoxazole (desktop)\n",
      "No data for Trimethoprim/sulfamethoxazole (mobile-web or mobile-app)\n",
      "No data for Trimethoprim/sulfamethoxazole (all-access)\n"
     ]
    }
   ],
   "source": [
    "#Iterate over each article and fetch pageview data for different access types\n",
    "for page in pages:\n",
    "    # Fetch desktop views\n",
    "    desktop_views = request_pageviews_per_article(article_title=page, access_type=\"desktop\")\n",
    "    if desktop_views and 'items' in desktop_views:\n",
    "        for month in desktop_views['items']:\n",
    "            month_data = {\n",
    "                \"article_title\": page,\n",
    "                \"timestamp\": month['timestamp'],\n",
    "                \"views\": month['views']\n",
    "            }\n",
    "            desktop_data.append(month_data)\n",
    "    else:\n",
    "        print(f\"No data for {page} (desktop)\")\n",
    "\n",
    "    # Fetch mobile views (sum mobile-web and mobile-app)\n",
    "    mobile_web_views = request_pageviews_per_article(article_title=page, access_type=\"mobile-web\")\n",
    "    mobile_app_views = request_pageviews_per_article(article_title=page, access_type=\"mobile-app\")\n",
    "    \n",
    "    if mobile_web_views and 'items' in mobile_web_views and mobile_app_views and 'items' in mobile_app_views:\n",
    "        for web_month, app_month in zip(mobile_web_views['items'], mobile_app_views['items']):\n",
    "            if web_month['timestamp'] == app_month['timestamp']:  # Ensure the timestamps match\n",
    "                month_data = {\n",
    "                    \"article_title\": page,\n",
    "                    \"timestamp\": web_month['timestamp'],\n",
    "                    \"views\": web_month['views'] + app_month['views']  # Sum mobile-web and mobile-app views\n",
    "                }\n",
    "                mobile_data.append(month_data)\n",
    "    else:\n",
    "        print(f\"No data for {page} (mobile-web or mobile-app)\")\n",
    "\n",
    "    # Fetch cumulative views (all-access)\n",
    "    cumulative_views = request_pageviews_per_article(article_title=page, access_type=\"all-access\")\n",
    "    if cumulative_views and 'items' in cumulative_views:\n",
    "        for month in cumulative_views['items']:\n",
    "            month_data = {\n",
    "                \"article_title\": page,\n",
    "                \"timestamp\": month['timestamp'],\n",
    "                \"views\": month['views']\n",
    "            }\n",
    "            cumulative_data.append(month_data)\n",
    "    else:\n",
    "        print(f\"No data for {page} (all-access)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Collected Data into DataFrames\n",
    "\n",
    "After collecting the pageview data for desktop, mobile, and cumulative views, the data is converted into `pandas` DataFrames for easier manipulation and analysis.\n",
    "\n",
    "### Steps:\n",
    "1. **Desktop Data**:\n",
    "   - The list `desktop_data`, which contains pageviews for desktop access, is converted into a DataFrame `df_desktop`.\n",
    "\n",
    "2. **Mobile Data**:\n",
    "   - The list `mobile_data`, containing combined mobile-web and mobile-app views, is converted into a DataFrame `df_mobile`.\n",
    "\n",
    "3. **Cumulative Data**:\n",
    "   - The `cumulative_data` list, storing total pageviews across all access types, is converted into a DataFrame `df_cumulative`.\n",
    "\n",
    "These DataFrames will allow for efficient data manipulation, analysis, and visualization in subsequent steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the collected data into DataFrames\n",
    "df_desktop = pd.DataFrame(desktop_data)\n",
    "df_mobile = pd.DataFrame(mobile_data)\n",
    "df_cumulative = pd.DataFrame(cumulative_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Results to JSON Files\n",
    "\n",
    "After converting the collected pageview data into DataFrames, the data is saved to JSON files for future use or sharing. Each DataFrame (desktop, mobile, cumulative) is exported as a separate JSON file.\n",
    "\n",
    "### Steps:\n",
    "1. **Desktop Data**:\n",
    "   - The `df_desktop` DataFrame is saved as `rare-disease_monthly_desktop_201507-202409.json` in the specified directory, formatted for readability with indentation.\n",
    "\n",
    "2. **Mobile Data**:\n",
    "   - The `df_mobile` DataFrame is saved as `rare-disease_monthly_mobile_201507-202409.json`, with records organized and indented.\n",
    "\n",
    "3. **Cumulative Data**:\n",
    "   - The `df_cumulative` DataFrame is saved as `rare-disease_monthly_cumulative_201507-202409.json` in the same structured format.\n",
    "\n",
    "These JSON files contain the monthly pageview data across different access types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the results to separate JSON files\n",
    "df_desktop.to_json(\"../data/rare-disease_monthly_desktop_201507-202409.json\", orient='records', indent=4)\n",
    "df_mobile.to_json(\"../data/rare-disease_monthly_mobile_201507-202409.json\", orient='records', indent=4)\n",
    "df_cumulative.to_json(\"../data/rare-disease_monthly_cumulative_201507-202409.json\", orient='records', indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
